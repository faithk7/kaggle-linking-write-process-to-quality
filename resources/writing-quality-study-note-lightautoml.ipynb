{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"},{"sourceId":6973319,"sourceType":"datasetVersion","datasetId":3949123},{"sourceId":150384981,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size=4><b>Created by yunsuxiaozi</b></font>\n\n<font size=5><b>Linking Writing Processes to Writing Quality</b></font>\n","metadata":{}},{"cell_type":"markdown","source":"## Import data and necessary libraries","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/code/cody11null/lgbm-x2-nn#LightAutoML-NN-(DenseLight)-prediction\n\n#安装包,--no-index 表示不从 PyPI（Python Package Index）上下载安装包\n#-U是upgrade -q是减少输出信息,--find-links=是指定链接 后面是安装的版本.\n!pip install --no-index -U -q --find-links=/kaggle/input/lightautoml-038-dependecies lightautoml==0.3.8\n!pip install --no-index -U -q --find-links=/kaggle/input/lightautoml-038-dependecies pandas==2.0.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T11:52:13.772909Z","iopub.execute_input":"2023-12-03T11:52:13.773531Z","iopub.status.idle":"2023-12-03T11:52:50.003274Z","shell.execute_reply.started":"2023-12-03T11:52:13.773477Z","shell.execute_reply":"2023-12-03T11:52:50.002232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd#导入csv文件的库\nimport numpy as np#进行矩阵运算的库\nimport gc#垃圾回收模块\n\nimport re#用于正则表达式提取\nfrom collections import Counter#用于对一组元素计数\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML#自动化表格机器学习模型的库\nfrom lightautoml.tasks import Task#定义机器学习任务的库\n\n#设置随机种子,保证模型可以复现\nimport random\nnp.random.seed(2023)\nrandom.seed(2023)\n#import optuna#自动超参数优化软件框架\nimport warnings#避免一些可以忽略的报错\nwarnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:52:50.006084Z","iopub.execute_input":"2023-12-03T11:52:50.006481Z","iopub.status.idle":"2023-12-03T11:53:30.078369Z","shell.execute_reply.started":"2023-12-03T11:52:50.006444Z","shell.execute_reply":"2023-12-03T11:53:30.077143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\")\nprint(f\"len(train_logs):{len(train_logs)}\")\ntrain_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:53:30.080001Z","iopub.execute_input":"2023-12-03T11:53:30.080741Z","iopub.status.idle":"2023-12-03T11:53:43.223129Z","shell.execute_reply.started":"2023-12-03T11:53:30.080705Z","shell.execute_reply":"2023-12-03T11:53:43.222035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_scores=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\")\nprint(f\"len(train_scores):{len(train_scores)}\")\ntrain_scores.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:53:43.225855Z","iopub.execute_input":"2023-12-03T11:53:43.226229Z","iopub.status.idle":"2023-12-03T11:53:43.246372Z","shell.execute_reply.started":"2023-12-03T11:53:43.226197Z","shell.execute_reply":"2023-12-03T11:53:43.245094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.merge(train_logs,train_scores,on=\"id\",how=\"left\")\nprint(f\"len(train_df):{len(train_df)}\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:53:43.247818Z","iopub.execute_input":"2023-12-03T11:53:43.248159Z","iopub.status.idle":"2023-12-03T11:53:45.760005Z","shell.execute_reply.started":"2023-12-03T11:53:43.24813Z","shell.execute_reply":"2023-12-03T11:53:45.758747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#统计‘q’ '.' ‘ ’的一个函数.\ndef getEssays(df):\n    #获取传入的df的这几列.\n    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change']]\n    #取出activity不等于'Nonproduction'的那些数据\n    textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n    #统计每个id的出现次数,不排序\n    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n    #最后的下标\n    lastIndex = 0\n    #创建一个新的序列对象.\n    essaySeries = pd.Series()\n    #index是第几个id,valCount是出现次数\n    for index, valCount in enumerate(valCountsArr):\n        #取出第i个id的['activity', 'cursor_position', 'text_change']\n        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change']].iloc[lastIndex : lastIndex + valCount]\n        #跳到下一个id的index\n        lastIndex += valCount\n        essayText = \"\"\n        for Input in currTextInput.values:\n            #input[0]是这个id的activity\n            if Input[0] == 'Replace':\n                #text_change按照' => '分开 replaceTxt:[' qqq qqqqq ', ' ']\n                replaceTxt = Input[2].split(' => ')#应该是A=>B的操作\n                #input[1]是鼠标位置,是一个数字 鼠标位置-len()\n                #这是一个字符串的转换操作,由replaceTxt[0]转成replaceTxt[1] \n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n            if Input[0] == 'Paste':#粘贴\n                #print(f\"input[2]:{Input[2]}\") #input[2]:qqqqqqqqqqq \n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n            if Input[0] == 'Remove/Cut':#删除剪切 在Input[1]的位置删除Input[2]\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n            #如果是Move from\n            if \"M\" in Input[0]:\n                #[284, 292] To [282, 290] 把[284, 292]这8行移动到[282,290]\n                croppedTxt = Input[0][10:]\n                #from和to的4个数字分开.\n                splitTxt = croppedTxt.split(' To ')\n                valueArr = [item.split(', ') for item in splitTxt]\n                moveData = (int(valueArr[0][0][1:]), \n                            int(valueArr[0][1][:-1]), \n                            int(valueArr[1][0][1:]), \n                            int(valueArr[1][1][:-1]))\n                #行号不相等,如果相等,等于什么都没有做\n                if moveData[0] != moveData[2]:\n                    #行号小于 \n                    if moveData[0] < moveData[2]:\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n                        essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    #行号大于\n                    else:\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n                        essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue\n            #相当于是个check    \n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n        #对应id对应论文\n        essaySeries[index] = essayText\n    #id\n    essaySeries.index =  textInputDf['id'].unique()\n    return pd.DataFrame(essaySeries, columns=['essay'])\n\n\ntrain_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\ntrain_essays.index = train_essays[\"Unnamed: 0\"]\ntrain_essays.index.name = None\ntrain_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\nprint(f\"len(train_essays):{len(train_essays)}\")\ntrain_essays['len_essay']=train_essays['essay'].apply(len)\ntrain_essays.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:53:45.761295Z","iopub.execute_input":"2023-12-03T11:53:45.761637Z","iopub.status.idle":"2023-12-03T11:53:45.861482Z","shell.execute_reply.started":"2023-12-03T11:53:45.761607Z","shell.execute_reply":"2023-12-03T11:53:45.86033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineer","metadata":{}},{"cell_type":"code","source":"#获取数据中第25%的数值\ndef q1(x):\n    return x.quantile(0.25)\n#获取数据中第75%的数值\ndef q3(x):\n    return x.quantile(0.75)\n#对数据聚合的操作:数量,均值,方差,最小值,最大值,第一,最后,均值标准误差,25%,50%,75%,偏斜度,峰度,求和\nAGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n\n#将传入的论文df转成句子\ndef split_essays_into_sentences(df):\n    essay_df = df#传入的df就是论文的df\n    essay_df['id'] = essay_df.index#index就是每个人的编号\n    #对句子按照. ? !进行拆分. 得到一个拆分后的列表.\n    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n    essay_df = essay_df.explode('sent')\n    #将换行符'\\n'变成空白字符 strip 去除行头和行尾的空白字符.\n    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n    #统计一下每个句子的长度 \n    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n    #求一下每个句子单词的个数.\n    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n    #去掉那些句子长度为0的数据\n    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n    return essay_df\n\n#对句子的一些特征进行统计.\ndef compute_sentence_aggregations(df):\n    #对每个id的句子长度和每个句子的单词个数进行统计学变量的统计.并且拼接在一起.\n    sent_agg_df = pd.concat(\n        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    )\n    #sent_agg_df.columns: (       'sent_len',  'count')-> 'sent_len_count'\n    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]#列名变换\n    sent_agg_df['id'] = sent_agg_df.index#将索引保存\n    sent_agg_df = sent_agg_df.reset_index(drop=True)#重置索引,删除index\n    #一句话里词的个数的count,其实就是有多少句话,也就是sent_len的count.重复了,故去掉.\n    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n    #sent_len_count其实就是有多少句话,故rename.\n    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n    return sent_agg_df\n\n#将论文根据换行符划分为段落.(每段有多少句话为什么没有统计?)\ndef split_essays_into_paragraphs(df):\n    essay_df = df\n    essay_df['id'] = essay_df.index\n    #按照'\\n'划分成段落 [1,2,3]\n    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n    #[论文1 [段落1 段落2,……]->[论文1 段落1 // 论文1 段落2]\n    essay_df = essay_df.explode('paragraph')\n    #统计段落的长度\n    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n    #统计每个段落的词数\n    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n    #将段落长度为0的数据去掉.\n    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n    return essay_df\n\n#对段落的长度和词数用统计学变量,和上面句子的代码一致.\ndef compute_paragraph_aggregations(df):\n    paragraph_agg_df = pd.concat(\n        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n    ) \n    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n    paragraph_agg_df['id'] = paragraph_agg_df.index\n    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n    return paragraph_agg_df\n#训练数据论文划分为句子\ntrain_sent_df = split_essays_into_sentences(train_essays)\n#句子特征提取\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n#论文划分成段落\ntrain_paragraph_df = split_essays_into_paragraphs(train_essays)\n#段落特征提取\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:53:45.863406Z","iopub.execute_input":"2023-12-03T11:53:45.863749Z","iopub.status.idle":"2023-12-03T11:54:02.371505Z","shell.execute_reply.started":"2023-12-03T11:53:45.863721Z","shell.execute_reply":"2023-12-03T11:54:02.370352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\")\nprint(f\"len(test_logs):{len(test_logs)}\")\n#测试数据得到论文,然后得到句子和段落的特征\ntest_essays = getEssays(test_logs)\ntest_essays['len_essay']=test_essays['essay'].apply(len)\ntest_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))\ntest_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:54:02.37282Z","iopub.execute_input":"2023-12-03T11:54:02.373128Z","iopub.status.idle":"2023-12-03T11:54:02.469346Z","shell.execute_reply.started":"2023-12-03T11:54:02.373101Z","shell.execute_reply":"2023-12-03T11:54:02.468179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict#一个存在默认值的字典,访问不存在的值时抛出的是默认值\n\nclass Preprocessor:#数据预处理的一个类\n    \n    def __init__(self, seed):\n        self.seed = seed#随机种子\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste','Move From']#这是activity的一列\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']#down_event中选出一些重要的\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']#text_change中选出一些重要的\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']#down_event中的一些标点符号\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]#滞后项\n        \n        #这里是用于存储每个activity的idf值\n        self.idf = defaultdict(float)#创建了一个float类型的字典,如果访问不存在,默认值为0.0\n    #将列表转成字符串\n    def list_to_str(self,lst):\n        return ','.join(lst)#按照','来进行拼接\n    def activity_each_word_speed(self,df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        tmp_df['activity']=tmp_df['activity'].apply(self.list_to_str)\n        #将逗号换成空格\n        tmp_df['activity']=tmp_df['activity'].apply(lambda x: x.replace(\",\",' '))\n\n        tmp_df['activity']=tmp_df['activity'].apply(lambda x: re.split('space',x))\n\n        tmp_df=tmp_df.explode('activity')\n\n        tmp_df['each_word_speed'] = tmp_df['activity'].apply(lambda x: len(x))\n\n        return tmp_df['each_word_speed'].groupby(tmp_df['id']).agg(AGGREGATIONS).reset_index().drop(['id'],axis=1,inplace=True)\n        \n    #统计df对象中activity的count\n    def activity_counts(self, df):\n        #对每个id的所有activity组合成一个列表\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        #创建一个空列表\n        ret = list()\n        for li in tmp_df['activity'].values:#取出一个人的activity列表\n            items = list(Counter(li).items())#转成[(activity1:count1),(activity2:count2),……]\n            di = dict()#一个空字典\n            #每个activity初始化为0\n            for k in self.activities:\n                di[k] = 0\n            #统计每个activity的count\n            for item in items:\n                k, v = item[0], item[1]#k:activity v:count\n                if k in di:\n                    di[k] = v\n            #加上这个人的每个activity的count\n            ret.append(di)\n        #转成pandas类型\n        ret = pd.DataFrame(ret)\n        #给表格的每列换个名字\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        #每列元素求和,文章中出现的总次数\n        cnts = ret.sum(1)\n\n        #前面是词袋模型,这里转成tf-idf模型\n        for col in cols:#activity_i_count\n            if col in self.idf.keys():#字典里如果已经有这个key了\n                idf = self.idf[col]\n            else:#不在这个字典里\n                #计算idf=log(数据量/(某列和+1))\n                idf = np.log(df.shape[0] / (ret[col].sum() + 1))\n                self.idf[col] = idf#将col的idf加入字典\n            #ret[col] / cnts :给定文章的次数/在文章中出现的总次数,为什么取log再加1不知道\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret#tf-idf\n\n    #这个是event的tf-idf模型,这里可能有down_event和up_event,故colname单独设置\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tmp_df[colname].values:\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n    #text_change的tf-idf模型\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tmp_df['text_change'].values:\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n    #统计标点之类的出现的次数,不过这次是直接将它们相加做统计的.(可能这样比tf-idf好?)\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tmp_df['down_event'].values:\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:#只要在这张表里,就相加\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n\n    def get_input_words(self, df):\n        #~是取反的布尔值 取出text_change 中不包含 => 且不是Nochange的\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        #在drop掉包含 => 和Nochange之后 按id打包成列表\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        #将列表连接成一个整体\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        #用正则表达式子匹配一个或者多个'q'字符\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        #统计len,也就是统计text_change中有多少个有q的字符\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        #求均值,方差,最大值,取到np.nan就设置为0\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    #对df做特征工程\n    def make_feats(self, df):\n        \n        print(\"Starting to engineer features\")\n        #创建一个只有id一列的表格\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        #做时序上的特征工程\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"-> for gap {gap}\")\n            #利用up_time的shift创造action_time_gap\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        #对cursor_position做特征工程,这个就是自己-自己\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            print(f\"-> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            #取了绝对值,鼠标向前移动也是移动了.\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        #对word_count做类似的特征工程,词数减少也是移动了.\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            print(f\"-> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        print(\"Engineering statistical summaries for features\")\n        #需要对哪些特征做哪些统计变量,这些都是大佬统计好的,就不做修改了.\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        #滞后特征的统计变量用for循环进行添加\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n            ])\n        \n        pbar = feats_stat\n        for item in pbar:\n            colname, methods = item[0], item[1]#取出某列特征和需要进行的统计学的量'max'\n            for method in methods:\n                #转成能放入agg的方法\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                #添加到feats里.\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        #调用方法求activity的tf-idf\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_each_word_speed(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        #调用方法求down_event和up_event的tf-idf\n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        # input words\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n\n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n        \n        print(\"Done!\")\n        return feats\npreprocessor = Preprocessor(seed=2023)\nprint(\"Engineering features for training data\")\n\ntrain_feats = preprocessor.make_feats(train_logs)\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\ntest_feats = preprocessor.make_feats(test_logs)\n#将训练数据中有缺失值的列作统计\nnan_cols = train_feats.columns[train_feats.isna().any()].tolist()\nprint(f\"len(nan_cols):{len(nan_cols)},nan_cols:{nan_cols}\")\n#将训练数据和测试数据去掉nan的列.\ntrain_feats = train_feats.drop(columns=nan_cols)\ntest_feats = test_feats.drop(columns=nan_cols)\n\nkeys=train_feats.keys().values\nunique_cols=[key for key in keys if train_feats[key].nunique()<2]\nprint(f\"len(unique_cols):{len(unique_cols)},unique_cols:{unique_cols}\")\n#将训练数据和测试数据去掉unique的列.\ntrain_feats = train_feats.drop(columns=unique_cols)\ntest_feats = test_feats.drop(columns=unique_cols)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:54:02.471429Z","iopub.execute_input":"2023-12-03T11:54:02.472446Z","iopub.status.idle":"2023-12-03T12:00:19.342981Z","shell.execute_reply.started":"2023-12-03T11:54:02.4724Z","shell.execute_reply":"2023-12-03T12:00:19.341272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#根据id对某些特征求了一些统计学变量\ntrain_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n#列名转成:\"特征_统计学量\"\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n#给列名添加上前缀tmp_\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n#将行号设置为[0,1,2,3,……]\ntrain_agg_fe_df.reset_index(inplace=True)\n\n#对测试集做同样的操作\ntest_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)\n\n#将文件合并起来\ntrain_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\ntest_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:00:19.347157Z","iopub.execute_input":"2023-12-03T12:00:19.34759Z","iopub.status.idle":"2023-12-03T12:00:27.76723Z","shell.execute_reply.started":"2023-12-03T12:00:19.347556Z","shell.execute_reply":"2023-12-03T12:00:27.765843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\n\nfor logs in [train_logs, test_logs]:\n    #up_time向后移动并且用down_time填充缺失的位置\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    #(down_time减上一个时刻的up_time) /1000是单位转换\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n\n    group = logs.groupby('id')['time_diff']\n    #延迟时间的max,min,median\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    #down_time的first /1000是做单位转换吧\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    #分层次求和\n    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x <= 1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x <= 1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x <= 2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x <= 3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n\n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        #延迟\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ngc.collect()\n\n#将延时的特征加入\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\n#在训练的feature中加入标签\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')\n\n#加上句子特征和段落特征.\ntrain_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\ntrain_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\ntrain_feats['len_essay']=train_essays['len_essay'].values\ntest_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\ntest_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')\ntest_feats['len_essay']=test_essays['len_essay'].values\n#id这列就去掉了\ntrain_feats.drop(['id'],axis=1,inplace=True)\ntest_feats.drop(['id'],axis=1,inplace=True)\ntrain_feats.fillna(0,inplace=True)\ntest_feats.fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:00:27.769589Z","iopub.execute_input":"2023-12-03T12:00:27.770119Z","iopub.status.idle":"2023-12-03T12:00:43.005797Z","shell.execute_reply.started":"2023-12-03T12:00:27.770078Z","shell.execute_reply":"2023-12-03T12:00:43.004574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"very_low_score=train_feats[train_feats['score']==0.5]\nlow_score=train_feats[train_feats['score']==1.0]\nhigh_score=train_feats[train_feats['score']==5.5]\nvery_high_score=train_feats[train_feats['score']==6]\nprint(f\"very_low_score['len_essay'].values:{very_low_score['len_essay'].values}\")\nprint(f\"low_score['len_essay'].values:{low_score['len_essay'].values}\")\nprint(f\"very_high_score['len_essay'].values:{very_high_score['len_essay'].values}\")\nprint(f\"high_score['len_essay'].values:{high_score['len_essay'].values}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:00:43.007295Z","iopub.execute_input":"2023-12-03T12:00:43.00769Z","iopub.status.idle":"2023-12-03T12:00:43.0518Z","shell.execute_reply.started":"2023-12-03T12:00:43.007659Z","shell.execute_reply":"2023-12-03T12:00:43.050656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## lightAutoml","metadata":{}},{"cell_type":"code","source":"N_THREADS = 4\nN_FOLDS = 5\nTEST_SIZE = 0.2\nTIMEOUT = 5 * 3600\nTARGET_NAME='score'\n#评估指标是RMSE\ndef RMSE(y_true,y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n#任务是回归任务,损失是MSE,评估指标是RMSE\ntask = Task('reg', loss = 'mse', metric = RMSE)\nroles = {\n    'target': TARGET_NAME\n}\nautoml = TabularAutoML(\n    task = task, \n    timeout = TIMEOUT,\n    cpu_limit = N_THREADS,\n    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': 2023, 'advanced_roles': False}\n)\n#%%time \noof_pred = automl.fit_predict(train_feats, roles = roles, verbose = 3)\nprint(automl.create_model_str_desc())\nprint(f'TRAIN out-of-fold score: {RMSE(train_feats[TARGET_NAME].values, oof_pred.data[:, 0])}')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:00:43.053413Z","iopub.execute_input":"2023-12-03T12:00:43.053775Z","iopub.status.idle":"2023-12-03T12:01:27.718965Z","shell.execute_reply.started":"2023-12-03T12:00:43.053744Z","shell.execute_reply":"2023-12-03T12:01:27.716822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pseudo label","metadata":{}},{"cell_type":"code","source":"test_pred=automl.predict(test_feats).data.T[0]\ntest_feats[TARGET_NAME] =test_pred\ntotal_feats=pd.concat((train_feats,test_feats),axis=0)\n\n#这是以前的参数. https://www.kaggle.com/code/yunsuxiaozi/crazy-feature-engineer-make-me-crazy\nlgbm_params1={'random_state': 1730, 'n_estimators': 228,#428, \n              'reg_alpha': 0.09004045282744186, 'reg_lambda': 4.173656114633056, \n              'colsample_bytree': 0.8713036303176558, 'subsample': 0.8184951070958928, \n              'learning_rate': 0.08775936917066098, 'num_leaves': 8, 'min_child_samples': 100}\n\nlgbm_params2= {'random_state': 1094, 'n_estimators': 189,#489, \n              'reg_alpha': 0.0038009385912019253, 'reg_lambda': 2.440515046794778,\n              'colsample_bytree': 0.5623454637024369, 'subsample': 0.7745734180496221, \n              'learning_rate': 0.15375733522524826, 'num_leaves': 13, \n              'min_child_samples': 87}\n\n\n\nfrom sklearn.model_selection import KFold#在机器学习库中导入k折交叉验证的函数\nfrom lightgbm import  LGBMRegressor \n\ndef RMSE(y_true,y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\nprint(\"start fit.\")\nfolds = 10#将数据分成10份\ny=total_feats['score']\nX=total_feats.drop(['score'],axis=1)\n\ntrain_RMSE=[]\nvalid_RMSE=[]\n# 存储已学习模型的列表\nmodels = []\n\n#将数据集随机打乱,并分成folds份\nkf = KFold(n_splits=folds, shuffle=True, random_state=2023) \n\n#从x_train中按照9:1的比例分成训练集和验证集,并取出下标\nfor train_index, valid_index in kf.split(X):\n    \n    #根据下标取出训练集和验证集的数据\n    x_train_cv = X.iloc[train_index]\n    y_train_cv = y.iloc[train_index]\n    x_valid_cv =X.iloc[valid_index]\n    y_valid_cv = y.iloc[valid_index]\n    \n    model = LGBMRegressor(**lgbm_params1)\n    \n    #模型用x_train_cv去训练,用x_train_cv和x_valid_cv一起去评估\n    model.fit(\n        x_train_cv,y_train_cv,\n        eval_set = [(x_train_cv, y_train_cv), (x_valid_cv, y_valid_cv)], \n        early_stopping_rounds=100,\n        verbose = 100, #迭代100次输出一个结果\n    )\n    \n    #对训练集进行预测\n    y_pred_train = model.predict(x_train_cv)        \n    #对验证集进行预测\n    y_pred_valid = model.predict(x_valid_cv) \n    \n        \n    y_pred_train=np.where(y_pred_train<=0.5,0.5,y_pred_train)\n    y_pred_train=np.where(y_pred_train>=6,6,y_pred_train)    \n    \n    y_pred_valid=np.where(y_pred_valid<=0.5,0.5,y_pred_valid)\n    y_pred_valid=np.where(y_pred_valid>=6,6,y_pred_valid)\n    \n    train_rmse=RMSE(y_pred_train,y_train_cv)\n    valid_rmse=RMSE(y_pred_valid,y_valid_cv)\n\n    \n    train_RMSE.append(train_rmse)\n    valid_RMSE.append(valid_rmse)\n    \n    #将model保存进列表中\n    models.append(model)\n    \n    model = LGBMRegressor(**lgbm_params2)\n    \n    #模型用x_train_cv去训练,用x_train_cv和x_valid_cv一起去评估\n    model.fit(\n        x_train_cv,y_train_cv,\n        eval_set = [(x_train_cv, y_train_cv), (x_valid_cv, y_valid_cv)], \n        early_stopping_rounds=100,\n        verbose = 100, #迭代100次输出一个结果\n    )\n    \n    #对训练集进行预测\n    y_pred_train = model.predict(x_train_cv)        \n    #对验证集进行预测\n    y_pred_valid = model.predict(x_valid_cv) \n    \n        \n    y_pred_train=np.where(y_pred_train<=0.5,0.5,y_pred_train)\n    y_pred_train=np.where(y_pred_train>=6,6,y_pred_train)    \n    \n    y_pred_valid=np.where(y_pred_valid<=0.5,0.5,y_pred_valid)\n    y_pred_valid=np.where(y_pred_valid>=6,6,y_pred_valid)\n    \n    train_rmse=RMSE(y_pred_train,y_train_cv)\n    valid_rmse=RMSE(y_pred_valid,y_valid_cv)\n\n    \n    train_RMSE.append(train_rmse)\n    valid_RMSE.append(valid_rmse)\n    \n    #将model保存进列表中\n    models.append(model)\n    \n    print(f\"train_RMSE:{train_RMSE},valid_RMSE:{valid_RMSE}\")\n\ntrain_RMSE=np.array(train_RMSE)\nvalid_RMSE=np.array(valid_RMSE)\n\nprint(f\"mean_train_RMSE:{np.mean(train_RMSE)}\")\nprint(f\"mean_valid_RMSE:{np.mean(valid_RMSE)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"test_X=test_feats.drop(['score'],axis=1).values\n#用每个保存的模型都对x_test预测一次,然后取平均值\npreds_test = []\n\nfor model in models:\n    \n    pred = model.predict(test_X)\n    \n    preds_test.append(pred)\n    \n#将预测结果转成np.array\npreds_test_np = np.array(preds_test)\n#按列对每个模型的预测结果取平均值\ntest_pred=np.mean(preds_test_np,axis=0)\ntest_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\")\nsubmission[TARGET_NAME] =test_pred\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:01:27.720962Z","iopub.status.idle":"2023-12-03T12:01:27.721692Z","shell.execute_reply.started":"2023-12-03T12:01:27.721354Z","shell.execute_reply":"2023-12-03T12:01:27.721401Z"},"trusted":true},"execution_count":null,"outputs":[]}]}